{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5e8ac0-e3f2-4bf5-a110-4d7c62b7bfee",
   "metadata": {},
   "source": [
    "# Data Pre- Processing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcddf4e-9009-43dd-87db-e99c2078d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import time # Import the time module\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1337b7ad-4b5b-4670-a948-174deaccdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EDA_filtered_Rating_Amazon_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6128b53-0ba5-4756-94f5-07f199fd2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1800000, 15)\n",
      "Target shape: (1800000,)\n"
     ]
    }
   ],
   "source": [
    "# Step1 defining features and target\n",
    "\n",
    "# Define the target variable (y) and features (X)\n",
    "# The `Rating_Sentiment` column appears to be the target variable based on the data structure.\n",
    "# Features will be all numerical columns from 'Review_str_len' to the end.\n",
    "#X = data.loc[:, 'Review_str_len':'years']\n",
    "# Drop non-numeric columns except target/label\n",
    "X = data.drop(['Rating_Sentiment'], axis=1)\n",
    "y = data['Rating_Sentiment']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bdd080-31e4-4eda-9708-315ac063b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes: ['Negative' 'Neutral' 'Positive']\n",
      "Encoded labels: [0 1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"Original classes: {le.classes_}\")\n",
    "print(f\"Encoded labels: {np.unique(y_encoded)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b45d3d2-9f5c-4dda-814c-4198554b5cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (1260000, 15)\n",
      "Validation features shape: (540000, 15)\n",
      "Training target shape: (1260000,)\n",
      "Validation target shape: (540000,)\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Split the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Validation target shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ede4991-5060-4cae-b34f-978b389399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = 'WordNet_Lemmatizer'\n",
    "numerical_features = ['Review_str_len', 'Title_str_len', 'Review_wtoken_cnt', 'lexical_diversity', 'review_removed_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9dbc3f0-f4d8-45d6-a664-beda030a9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing data transformations...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Define the shared preprocessors and pre-compute data ---\n",
    "\n",
    "# Preprocessor for all models except Naive Bayes\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=500, stop_words='english'), text_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Preprocessor specifically for Naive Bayes to handle non-negative inputs\n",
    "preprocessor_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=500, stop_words='english'), text_features),\n",
    "        ('num', MinMaxScaler(), numerical_features) # Using MinMaxScaler to prevent negative values\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 2.1 Pre-compute the transformed data ONCE ---\n",
    "\n",
    "print(\"Pre-computing data transformations...\")\n",
    "\n",
    "# Data for Implementation 1: TF-IDF + StandardScaler\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "# Data for Implementation 2 & 3: TF-IDF + StandardScaler + PCA\n",
    "# We must first fit the preprocessor, then the PCA on the preprocessed data\n",
    "pipeline_PCA = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=83)),\n",
    "])\n",
    "X_train_pca = pipeline_PCA.fit_transform(X_train, y_train)\n",
    "X_val_pca = pipeline_PCA.transform(X_val)\n",
    "\n",
    "# Data for Naive Bayes models (pre-computed separately to handle non-negativity)\n",
    "X_train_nb = preprocessor_nb.fit_transform(X_train)\n",
    "X_val_nb = preprocessor_nb.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021fe348-eb37-40cf-8558-d7a9f74b52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Create a dictionary of models to experiment with ---\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    #'KNN': KNeighborsClassifier(),\n",
    "    #'NaiveBayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# --- 4. Define parameter grids for each model ---\n",
    "param_grids = {\n",
    "   'LogisticRegression': {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [10, 20, None]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model__n_neighbors': [3, 5, 7],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'model__alpha': [0.1, 0.5, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7be064-7b21-45a3-8bcf-3c0dcc1d3e83",
   "metadata": {},
   "source": [
    "# Implementation 1: Pipeline(TF-IDF + StandardScaler -> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de63adf5-8420-49e4-9440-81894960093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation is split to have LogisticRegression, XGBoost, RandomForst and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf392f2c-2fda-4366-b483-534dc305f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Basic Implementations ---\n",
    "\n",
    "print(\"\\n--- Implementation 1: Pipeline(TF-IDF + StandardScaler -> Model) ---\")\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nTraining and evaluating the {model_name} model...\")\n",
    "    try:\n",
    "        if model_name == 'NaiveBayes':\n",
    "            model_instance.fit(X_train_nb, y_train)\n",
    "            score = model_instance.score(X_val_nb, y_val)\n",
    "        else:\n",
    "            model_instance.fit(X_train_transformed, y_train)\n",
    "            score = model_instance.score(X_val_transformed, y_val)\n",
    "        \n",
    "        print(f\"Accuracy for {model_name}: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1df5b0c9-ad4c-48a8-972e-3ca3aaf885ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 1: Pipeline(TF-IDF + StandardScaler -> Model) ---\n",
      "\n",
      "Training and evaluating the  'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
      "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
      "    'RandomForest': RandomForestClassifier(random_state=42), KNN model...\n",
      "Accuracy for  'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
      "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
      "    'RandomForest': RandomForestClassifier(random_state=42), KNN: 0.4560\n",
      "\n",
      "Training and evaluating the NaiveBayes model...\n",
      "Accuracy for NaiveBayes: 0.6347\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Implementations ---\n",
    "\n",
    "print(\"\\n--- Implementation 1: Pipeline(TF-IDF + StandardScaler -> Model) ---\")\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nTraining and evaluating the {model_name} model...\")\n",
    "    try:\n",
    "        if model_name == 'NaiveBayes':\n",
    "            model_instance.fit(X_train_nb, y_train)\n",
    "            score = model_instance.score(X_val_nb, y_val)\n",
    "        else:\n",
    "            model_instance.fit(X_train_transformed, y_train)\n",
    "            score = model_instance.score(X_val_transformed, y_val)\n",
    "        \n",
    "        print(f\"Accuracy for {model_name}: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3c0d1-8aec-4375-a688-cb09ed7b0067",
   "metadata": {},
   "source": [
    "# Implementation 2: Pipeline(TF-IDF + StandardScaler -> PCA -> Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df67c265-92ac-4574-835f-f46a55b4f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 2: Pipeline(TF-IDF + StandardScaler -> PCA -> Model) ---\n",
      "\n",
      "Training and evaluating the LogisticRegression model...\n",
      "Accuracy for LogisticRegression: 0.6112\n",
      "\n",
      "Training and evaluating the XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:46:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost: 0.6220\n",
      "\n",
      "Training and evaluating the RandomForest model...\n",
      "Accuracy for RandomForest: 0.5974\n",
      "\n",
      "Training and evaluating the KNN model...\n",
      "Accuracy for KNN: 0.4680\n",
      "\n",
      "Training and evaluating the NaiveBayes model...\n",
      "Skipping NaiveBayes as it is incompatible with PCA.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 2: Pipeline(TF-IDF + StandardScaler -> PCA -> Model) ---\")\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nTraining and evaluating the {model_name} model...\")\n",
    "    try:\n",
    "        if model_name == 'NaiveBayes':\n",
    "            # Naive Bayes and PCA are incompatible because PCA produces negative values\n",
    "            print(f\"Skipping {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "        \n",
    "        model_instance.fit(X_train_pca, y_train)\n",
    "        score = model_instance.score(X_val_pca, y_val)\n",
    "        \n",
    "        print(f\"Accuracy for {model_name}: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1da4ca-b521-4b5a-82b2-3b127bc62a32",
   "metadata": {},
   "source": [
    "# Implementation 3: Pipeline(TF-IDF + StandardScaler -> PCA -> Model) with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10196c9-337d-479d-bffc-87c54dfff0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 3: Pipeline(TF-IDF + StandardScaler -> PCA -> Model) with GridSearchCV ---\n",
      "\n",
      "Performing GridSearchCV for the LogisticRegression model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'model__C': 1.0, 'model__solver': 'lbfgs'}\n",
      "Validation Accuracy for the best LogisticRegression model: 0.6112\n",
      "\n",
      "Performing GridSearchCV for the XGBoost model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'model__learning_rate': 0.2, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best XGBoost model: 0.6267\n",
      "\n",
      "Performing GridSearchCV for the RandomForest model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'model__max_depth': None, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best RandomForest model: 0.6018\n",
      "\n",
      "Performing GridSearchCV for the KNN model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters for KNN: {'model__n_neighbors': 7, 'model__weights': 'distance'}\n",
      "Validation Accuracy for the best KNN model: 0.4844\n",
      "\n",
      "Performing GridSearchCV for the NaiveBayes model...\n",
      "Skipping GridSearchCV for NaiveBayes as it is incompatible with PCA.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 3: Pipeline(TF-IDF + StandardScaler -> PCA -> Model) with GridSearchCV ---\")\n",
    "best_models = {}\n",
    "\n",
    "# The PCA pipeline is defined outside the loop\n",
    "simplified_pca_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=83)),\n",
    "])\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for the {model_name} model...\")\n",
    "    \n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    if not param_grid:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create the full pipeline for GridSearchCV\n",
    "    # The preprocessor is no longer in the pipeline! It's pre-computed.\n",
    "    full_pipeline_gs = Pipeline([\n",
    "        ('model', model_instance)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Check for NaiveBayes and handle its incompatibility with PCA\n",
    "        if model_name == 'NaiveBayes':\n",
    "            print(f\"Skipping GridSearchCV for {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            full_pipeline_gs, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='accuracy', \n",
    "            n_jobs=4,  # Use a specific number of jobs to prevent memory overload\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV on the pre-computed PCA data\n",
    "        grid_search.fit(X_train_pca, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        best_score = grid_search.score(X_val_pca, y_val)\n",
    "        print(f\"Validation Accuracy for the best {model_name} model: {best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running GridSearchCV for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875295-e258-4721-be34-c17ccb0be731",
   "metadata": {},
   "source": [
    "# Implementation 4: Pipeline(TF-IDF + StandardScaler -> Model) with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eefd25-a815-4687-9584-f213fe48b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 4: Pipeline(TF-IDF + StandardScaler -> Model) with GridSearchCV ---\n",
      "\n",
      "Performing GridSearchCV for the LogisticRegression model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'model__C': 0.1, 'model__solver': 'lbfgs'}\n",
      "Validation Accuracy for the best LogisticRegression model: 0.6613\n",
      "\n",
      "Performing GridSearchCV for the XGBoost model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'model__learning_rate': 0.2, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best XGBoost model: 0.6654\n",
      "\n",
      "Performing GridSearchCV for the RandomForest model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 4: Pipeline(TF-IDF + StandardScaler -> Model) with GridSearchCV ---\")\n",
    "best_models = {}\n",
    "\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for the {model_name} model...\")\n",
    "    \n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    if not param_grid:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create the full pipeline for GridSearchCV\n",
    "    # The preprocessor is no longer in the pipeline! It's pre-computed.\n",
    "    full_pipeline_gs = Pipeline([\n",
    "        ('model', model_instance)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Check for NaiveBayes and handle its incompatibility with PCA\n",
    "        if model_name == 'NaiveBayes':\n",
    "            print(f\"Skipping GridSearchCV for {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            full_pipeline_gs, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='accuracy', \n",
    "            n_jobs=4,  # Use a specific number of jobs to prevent memory overload\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV on the pre-computed PCA data\n",
    "        grid_search.fit(X_train_nb, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        best_score = grid_search.score(X_val_nb, y_val)\n",
    "        print(f\"Validation Accuracy for the best {model_name} model: {best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running GridSearchCV for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb74aa8-7908-4c75-8761-e01bcd1f2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation is split to have RandomForst and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d85553-9b5c-4611-adb2-223f0cb91850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 5: Pipeline(TF-IDF Bigram and Trigram + StandardScaler -> Model) with GridSearchCV ---\n",
      "\n",
      "Performing GridSearchCV for the RandomForest model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'model__max_depth': None, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best RandomForest model: 0.6535\n",
      "\n",
      "Performing GridSearchCV for the KNN model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNN: {'model__n_neighbors': 7, 'model__weights': 'distance'}\n",
      "Validation Accuracy for the best KNN model: 0.4952\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 4: Pipeline(TF-IDF + StandardScaler -> Model) with GridSearchCV ---\")\n",
    "best_models = {}\n",
    "\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for the {model_name} model...\")\n",
    "    \n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    if not param_grid:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create the full pipeline for GridSearchCV\n",
    "    # The preprocessor is no longer in the pipeline! It's pre-computed.\n",
    "    full_pipeline_gs = Pipeline([\n",
    "        ('model', model_instance)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Check for NaiveBayes and handle its incompatibility with PCA\n",
    "        if model_name == 'NaiveBayes':\n",
    "            print(f\"Skipping GridSearchCV for {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            full_pipeline_gs, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='accuracy', \n",
    "            n_jobs=4,  # Use a specific number of jobs to prevent memory overload\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV on the pre-computed PCA data\n",
    "        grid_search.fit(X_train_nb, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        best_score = grid_search.score(X_val_nb, y_val)\n",
    "        print(f\"Validation Accuracy for the best {model_name} model: {best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running GridSearchCV for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615baf3c-7231-4e3c-b14c-6e983581718e",
   "metadata": {},
   "source": [
    "# Implmenetation with Bigram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ce35cc-4e5f-4b18-86d8-1dbd55ce6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing data transformations...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Define the shared preprocessors and pre-compute data ---\n",
    "\n",
    "# Preprocessor for all models except Naive Bayes\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Updated TfidfVectorizer to use bigrams and trigrams\n",
    "        ('text', TfidfVectorizer(max_features=500, stop_words='english', ngram_range=(2, 3)), text_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Preprocessor specifically for Naive Bayes to handle non-negative inputs\n",
    "preprocessor_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Updated TfidfVectorizer to use bigrams and trigrams\n",
    "        ('text', TfidfVectorizer(max_features=500, stop_words='english', ngram_range=(2, 3)), text_features),\n",
    "        ('num', MinMaxScaler(), numerical_features) # Using MinMaxScaler to prevent negative values\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 2.1 Pre-compute the transformed data ONCE ---\n",
    "\n",
    "print(\"Pre-computing data transformations...\")\n",
    "\n",
    "# Data for Implementation 1: TF-IDF + StandardScaler\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "# Data for Implementation 2 & 3: TF-IDF + StandardScaler + PCA\n",
    "# We must first fit the preprocessor, then the PCA on the preprocessed data\n",
    "pipeline_PCA = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=83)),\n",
    "])\n",
    "X_train_pca = pipeline_PCA.fit_transform(X_train, y_train)\n",
    "X_val_pca = pipeline_PCA.transform(X_val)\n",
    "\n",
    "# Data for Naive Bayes models (pre-computed separately to handle non-negativity)\n",
    "X_train_nb = preprocessor_nb.fit_transform(X_train)\n",
    "X_val_nb = preprocessor_nb.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0ce10b-c9ac-4c74-8258-aadc977e10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Create a dictionary of models to experiment with ---\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# --- 4. Define parameter grids for each model ---\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [10, 20, None]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model__n_neighbors': [3, 5, 7],\n",
    "        'model__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'model__alpha': [0.1, 0.5, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52679d9e-41f2-4fda-a4ff-be6ed477621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 5: Pipeline(TF-IDF Bigram and Trigram + StandardScaler -> Model) with GridSearchCV ---\n",
      "\n",
      "Performing GridSearchCV for the LogisticRegression model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'model__C': 0.1, 'model__solver': 'lbfgs'}\n",
      "Validation Accuracy for the best LogisticRegression model: 0.5181\n",
      "\n",
      "Performing GridSearchCV for the XGBoost model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'model__learning_rate': 0.2, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best XGBoost model: 0.5135\n",
      "\n",
      "Performing GridSearchCV for the RandomForest model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 5: Pipeline(TF-IDF Bigram and Trigram + StandardScaler -> Model) with GridSearchCV ---\")\n",
    "best_models = {}\n",
    "\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for the {model_name} model...\")\n",
    "    \n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    if not param_grid:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create the full pipeline for GridSearchCV\n",
    "    # The preprocessor is no longer in the pipeline! It's pre-computed.\n",
    "    full_pipeline_gs = Pipeline([\n",
    "        ('model', model_instance)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Check for NaiveBayes and handle its incompatibility with PCA\n",
    "        if model_name == 'NaiveBayes':\n",
    "            print(f\"Skipping GridSearchCV for {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            full_pipeline_gs, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='accuracy', \n",
    "            n_jobs=4,  # Use a specific number of jobs to prevent memory overload\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV on the pre-computed PCA data\n",
    "        grid_search.fit(X_train_nb, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        best_score = grid_search.score(X_val_nb, y_val)\n",
    "        print(f\"Validation Accuracy for the best {model_name} model: {best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running GridSearchCV for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80deda19-8156-45f2-8d19-4d7169af6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation is split to have RandomForst and KNN as it took longer time in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3179233-7663-440f-8aab-638783df042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementation 5: Pipeline(TF-IDF Bigram and Trigram + StandardScaler -> Model) with GridSearchCV ---\n",
      "\n",
      "Performing GridSearchCV for the RandomForest model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters for RandomForest: {'model__max_depth': None, 'model__n_estimators': 200}\n",
      "Validation Accuracy for the best RandomForest model: 0.4926\n",
      "\n",
      "Performing GridSearchCV for the KNN model...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters for KNN: {'model__n_neighbors': 7, 'model__weights': 'uniform'}\n",
      "Validation Accuracy for the best KNN model: 0.4730\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Implementation 5: Pipeline(TF-IDF Bigram and Trigram + StandardScaler -> Model) with GridSearchCV ---\")\n",
    "best_models = {}\n",
    "\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"\\nPerforming GridSearchCV for the {model_name} model...\")\n",
    "    \n",
    "    param_grid = param_grids.get(model_name, {})\n",
    "    if not param_grid:\n",
    "        print(f\"No parameter grid defined for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create the full pipeline for GridSearchCV\n",
    "    # The preprocessor is no longer in the pipeline! It's pre-computed.\n",
    "    full_pipeline_gs = Pipeline([\n",
    "        ('model', model_instance)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # Check for NaiveBayes and handle its incompatibility with PCA\n",
    "        if model_name == 'NaiveBayes':\n",
    "            print(f\"Skipping GridSearchCV for {model_name} as it is incompatible with PCA.\")\n",
    "            continue\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            full_pipeline_gs, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='accuracy', \n",
    "            n_jobs=4,  # Use a specific number of jobs to prevent memory overload\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit GridSearchCV on the pre-computed PCA data\n",
    "        grid_search.fit(X_train_nb, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        best_score = grid_search.score(X_val_nb, y_val)\n",
    "        print(f\"Validation Accuracy for the best {model_name} model: {best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running GridSearchCV for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8201706-b1c1-40e2-bd72-b535acba9006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Implementations Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- All Implementations Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
