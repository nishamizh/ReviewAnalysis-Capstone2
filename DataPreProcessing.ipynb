{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5e8ac0-e3f2-4bf5-a110-4d7c62b7bfee",
   "metadata": {},
   "source": [
    "# Data Pre- Processing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcddf4e-9009-43dd-87db-e99c2078d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5717d5a6-b707-4918-83b3-b6326ead318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the saved data from EDA\n",
    "\n",
    "data = pd.read_csv('EDA_reviews_amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f283ec4f-3798-49c7-bd6b-02eb83213686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rating_Sentiment</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_str_len</th>\n",
       "      <th>Title_str_len</th>\n",
       "      <th>title_word_tokenize</th>\n",
       "      <th>Review_word_tokenize</th>\n",
       "      <th>title_wtoken_cnt</th>\n",
       "      <th>Review_wtoken_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>works</th>\n",
       "      <th>worth</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>likert_scale</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>gave this to my dad for a gag gift after direc...</td>\n",
       "      <td>93</td>\n",
       "      <td>18</td>\n",
       "      <td>['more', 'like', 'funchuck']</td>\n",
       "      <td>['gave', 'this', 'to', 'my', 'dad', 'for', 'a'...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Average</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>-0.100980</td>\n",
       "      <td>-0.045847</td>\n",
       "      <td>0.022069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>inspiring</td>\n",
       "      <td>i hope a lot of people hear this cd  we need m...</td>\n",
       "      <td>204</td>\n",
       "      <td>9</td>\n",
       "      <td>['inspiring']</td>\n",
       "      <td>['i', 'hope', 'a', 'lot', 'of', 'people', 'hea...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>-0.085753</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>-0.122467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>the best soundtrack ever to anything</td>\n",
       "      <td>i m reading a lot of reviews saying that this ...</td>\n",
       "      <td>470</td>\n",
       "      <td>37</td>\n",
       "      <td>['the', 'best', 'soundtrack', 'ever', 'to', 'a...</td>\n",
       "      <td>['i', 'm', 'reading', 'a', 'lot', 'of', 'revie...</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248071</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>-0.113531</td>\n",
       "      <td>-0.050741</td>\n",
       "      <td>-0.071864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>too good to be true</td>\n",
       "      <td>probably the greatest soundtrack in history  u...</td>\n",
       "      <td>377</td>\n",
       "      <td>19</td>\n",
       "      <td>['too', 'good', 'to', 'be', 'true']</td>\n",
       "      <td>['probably', 'the', 'greatest', 'soundtrack', ...</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>-0.127917</td>\n",
       "      <td>-0.052078</td>\n",
       "      <td>0.012366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>there s a reason for the price</td>\n",
       "      <td>there s a reason this cd is so expensive  even...</td>\n",
       "      <td>193</td>\n",
       "      <td>30</td>\n",
       "      <td>['there', 's', 'a', 'reason', 'for', 'the', 'p...</td>\n",
       "      <td>['there', 's', 'a', 'reason', 'this', 'cd', 'i...</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.083067</td>\n",
       "      <td>-0.123744</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.144462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating Rating_Sentiment                           Review_Title  \\\n",
       "0       3          Neutral                     more like funchuck   \n",
       "1       5         Positive                              inspiring   \n",
       "2       5         Positive  the best soundtrack ever to anything    \n",
       "3       5         Positive                    too good to be true   \n",
       "4       5         Positive         there s a reason for the price   \n",
       "\n",
       "                                              Review  Review_str_len  \\\n",
       "0  gave this to my dad for a gag gift after direc...              93   \n",
       "1  i hope a lot of people hear this cd  we need m...             204   \n",
       "2  i m reading a lot of reviews saying that this ...             470   \n",
       "3  probably the greatest soundtrack in history  u...             377   \n",
       "4  there s a reason this cd is so expensive  even...             193   \n",
       "\n",
       "   Title_str_len                                title_word_tokenize  \\\n",
       "0             18                       ['more', 'like', 'funchuck']   \n",
       "1              9                                      ['inspiring']   \n",
       "2             37  ['the', 'best', 'soundtrack', 'ever', 'to', 'a...   \n",
       "3             19                ['too', 'good', 'to', 'be', 'true']   \n",
       "4             30  ['there', 's', 'a', 'reason', 'for', 'the', 'p...   \n",
       "\n",
       "                                Review_word_tokenize  title_wtoken_cnt  \\\n",
       "0  ['gave', 'this', 'to', 'my', 'dad', 'for', 'a'...                 3   \n",
       "1  ['i', 'hope', 'a', 'lot', 'of', 'people', 'hea...                 1   \n",
       "2  ['i', 'm', 'reading', 'a', 'lot', 'of', 'revie...                 6   \n",
       "3  ['probably', 'the', 'greatest', 'soundtrack', ...                 5   \n",
       "4  ['there', 's', 'a', 'reason', 'this', 'cd', 'i...                 7   \n",
       "\n",
       "   Review_wtoken_cnt  ... works     worth  written year     years  \\\n",
       "0                 20  ...   0.0  0.000000      0.0  0.0  0.000000   \n",
       "1                 38  ...   0.0  0.000000      0.0  0.0  0.000000   \n",
       "2                 96  ...   0.0  0.259375      0.0  0.0  0.248071   \n",
       "3                 67  ...   0.0  0.383432      0.0  0.0  0.000000   \n",
       "4                 41  ...   0.0  0.000000      0.0  0.0  0.000000   \n",
       "\n",
       "   likert_scale  lexical_diversity      PCA1      PCA2      PCA3  \n",
       "0       Average           0.160000 -0.100980 -0.045847  0.022069  \n",
       "1          Good           0.086538 -0.085753  0.003868 -0.122467  \n",
       "2          Good           0.035904 -0.113531 -0.050741 -0.071864  \n",
       "3          Good           0.051327 -0.127917 -0.052078  0.012366  \n",
       "4          Good           0.083067 -0.123744  0.000005 -0.144462  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6128b53-0ba5-4756-94f5-07f199fd2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1800000, 110)\n",
      "Target shape: (1800000,)\n"
     ]
    }
   ],
   "source": [
    "# Step1 defining features and target\n",
    "\n",
    "# Define the target variable (y) and features (X)\n",
    "# The `Rating_Sentiment` column appears to be the target variable based on the data structure.\n",
    "# Features will be all numerical columns from 'Review_str_len' to the end.\n",
    "#X = data.loc[:, 'Review_str_len':'years']\n",
    "# Drop non-numeric columns except target/label\n",
    "X = data.drop(['Review_Title', 'title_word_tokenize','Review_word_tokenize','Review', 'likert_scale', 'Cleaned_Review', 'review_punc_stop_words_removed', 'porterStemmer', 'WordNet_Lemmatizer', 'Rating_Sentiment'], axis=1)\n",
    "y = data['Rating_Sentiment']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc7bea5-1f77-4206-b3f3-23b379b53142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_str_len</th>\n",
       "      <th>Title_str_len</th>\n",
       "      <th>title_wtoken_cnt</th>\n",
       "      <th>Review_wtoken_cnt</th>\n",
       "      <th>review_removed_cnt</th>\n",
       "      <th>actually</th>\n",
       "      <th>album</th>\n",
       "      <th>author</th>\n",
       "      <th>bad</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>worth</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>-0.100980</td>\n",
       "      <td>-0.045847</td>\n",
       "      <td>0.022069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>204</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>-0.085753</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>-0.122467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>470</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.248071</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>-0.113531</td>\n",
       "      <td>-0.050741</td>\n",
       "      <td>-0.071864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>377</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>-0.127917</td>\n",
       "      <td>-0.052078</td>\n",
       "      <td>0.012366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>193</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083067</td>\n",
       "      <td>-0.123744</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.144462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating  Review_str_len  Title_str_len  title_wtoken_cnt  Review_wtoken_cnt  \\\n",
       "0       3              93             18                 3                 20   \n",
       "1       5             204              9                 1                 38   \n",
       "2       5             470             37                 6                 96   \n",
       "3       5             377             19                 5                 67   \n",
       "4       5             193             30                 7                 41   \n",
       "\n",
       "   review_removed_cnt  actually  album  author  bad  ...  work  works  \\\n",
       "0                   9       0.0    0.0     0.0  0.0  ...   0.0    0.0   \n",
       "1                  22       0.0    0.0     0.0  0.0  ...   0.0    0.0   \n",
       "2                  34       0.0    0.0     0.0  0.0  ...   0.0    0.0   \n",
       "3                  28       0.0    0.0     0.0  0.0  ...   0.0    0.0   \n",
       "4                  11       0.0    0.0     0.0  0.0  ...   0.0    0.0   \n",
       "\n",
       "      worth  written  year     years  lexical_diversity      PCA1      PCA2  \\\n",
       "0  0.000000      0.0   0.0  0.000000           0.160000 -0.100980 -0.045847   \n",
       "1  0.000000      0.0   0.0  0.000000           0.086538 -0.085753  0.003868   \n",
       "2  0.259375      0.0   0.0  0.248071           0.035904 -0.113531 -0.050741   \n",
       "3  0.383432      0.0   0.0  0.000000           0.051327 -0.127917 -0.052078   \n",
       "4  0.000000      0.0   0.0  0.000000           0.083067 -0.123744  0.000005   \n",
       "\n",
       "       PCA3  \n",
       "0  0.022069  \n",
       "1 -0.122467  \n",
       "2 -0.071864  \n",
       "3  0.012366  \n",
       "4 -0.144462  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b45d3d2-9f5c-4dda-814c-4198554b5cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (1440000, 110)\n",
      "Validation features shape: (360000, 110)\n",
      "Training target shape: (1440000,)\n",
      "Validation target shape: (360000,)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the data into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Validation target shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c77d5c-188e-43db-90bc-33af7e5aaf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled training features (first 5 rows):\n",
      "     Rating  Review_str_len  Title_str_len  title_wtoken_cnt  \\\n",
      "0  1.224796       -0.037667       0.092816         -0.160130   \n",
      "1 -1.225064        0.345768      -0.685000         -0.533372   \n",
      "2 -1.225064       -0.459446      -1.392106         -1.279855   \n",
      "3 -0.000134        0.648256       1.224186          1.332836   \n",
      "4 -1.225064       -0.796017      -0.755711         -0.906613   \n",
      "\n",
      "   Review_wtoken_cnt  review_removed_cnt  actually     album    author  \\\n",
      "0          -0.112532            0.092511 -0.179722 -0.215662 -0.188686   \n",
      "1           0.301066            0.516678  3.735406 -0.215662 -0.188686   \n",
      "2          -0.365287           -0.808842 -0.179722 -0.215662 -0.188686   \n",
      "3           0.415954            0.728761 -0.179722 -0.215662  7.363296   \n",
      "4          -0.916751           -0.808842 -0.179722 -0.215662 -0.188686   \n",
      "\n",
      "        bad  ...      work     works     worth   written      year     years  \\\n",
      "0 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "1 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "2 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435  2.786606   \n",
      "3 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "4 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "\n",
      "   lexical_diversity      PCA1      PCA2      PCA3  \n",
      "0          -0.448695 -0.627912  0.035228 -1.551609  \n",
      "1          -0.625915  0.957683 -0.105283 -0.075708  \n",
      "2          -0.164195 -0.838209  3.930468  2.057377  \n",
      "3          -0.732348  1.321569 -0.081343  0.034002  \n",
      "4           0.594332 -0.557348 -0.113117  0.266300  \n",
      "\n",
      "[5 rows x 110 columns]\n",
      "\n",
      "Scaled validation features (first 5 rows):\n",
      "     Rating  Review_str_len  Title_str_len  title_wtoken_cnt  \\\n",
      "0 -0.000134        1.653708      -0.260737          0.213111   \n",
      "1  1.224796       -1.294483      -0.967843         -0.906613   \n",
      "2 -1.225064       -1.162411       0.587790          0.586353   \n",
      "3 -0.000134        2.420578      -0.472868          0.213111   \n",
      "4 -1.225064        0.196654       0.729212          0.213111   \n",
      "\n",
      "   Review_wtoken_cnt  review_removed_cnt  actually     album    author  \\\n",
      "0           1.564837            1.789178 -0.179722 -0.215662 -0.188686   \n",
      "1          -1.261416           -1.339051 -0.179722 -0.215662 -0.188686   \n",
      "2          -1.192483           -1.020926 -0.179722 -0.215662 -0.188686   \n",
      "3           2.185234            2.478448 -0.179722 -0.215662 -0.188686   \n",
      "4           0.140222            0.145532 -0.179722 -0.215662 -0.188686   \n",
      "\n",
      "        bad  ...      work     works     worth   written      year     years  \\\n",
      "0 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "1 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "2 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "3 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "4 -0.232802  ... -0.274708 -0.193677 -0.211024 -0.185327 -0.198435 -0.238612   \n",
      "\n",
      "   lexical_diversity      PCA1      PCA2      PCA3  \n",
      "0          -0.962791 -0.188970  0.157182 -0.792242  \n",
      "1           2.259994 -0.339030 -0.102206  0.129691  \n",
      "2           2.067027 -0.339030 -0.102206  0.129691  \n",
      "3          -1.128633  0.468079 -0.042763  0.001450  \n",
      "4          -0.616195 -0.836945 -1.864613  2.549273  \n",
      "\n",
      "[5 rows x 110 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Normalize and standardize the numerical features\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and validation data\n",
    "# The scaler learns the mean and standard deviation from the training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame for easier inspection\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Scaled training features (first 5 rows):\")\n",
    "print(X_train_scaled_df.head())\n",
    "\n",
    "print(\"\\nScaled validation features (first 5 rows):\")\n",
    "print(X_val_scaled_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
